@incollection{NIPS2015_5651,
    author = {Berglund, Mathias and Raiko, Tapani and Honkala, Mikko and K\"{a}rkk\"{a}inen, Leo and Vetek, Akos and Karhunen, Juha T.},
    booktitle = {Advances in Neural Information Processing Systems 28},
    citeulike-article-id = {14575944},
    citeulike-linkout-0 = {http://papers.nips.cc/paper/5651-bidirectional-recurrent-neural-networks-as-generative-models.pdf},
    editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
    pages = {856--864},
    posted-at = {2018-04-27 00:35:31},
    priority = {2},
    publisher = {Curran Associates, Inc.},
    title = {Bidirectional Recurrent Neural Networks as Generative Models},
    url = {http://papers.nips.cc/paper/5651-bidirectional-recurrent-neural-networks-as-generative-models.pdf},
    year = {2015}
}

@incollection{NIPS2001_1996,
    author = {Platt, John C. and Burges, Christopher J. C. and Swenson, Steven and Weare, Christopher and Zheng, Alice},
    booktitle = {Advances in Neural Information Processing Systems 14},
    citeulike-article-id = {14575943},
    citeulike-linkout-0 = {http://papers.nips.cc/paper/1996-learning-a-gaussian-process-prior-for-automatically-generating-music-playlists.pdf},
    editor = {Dietterich, T. G. and Becker, S. and Ghahramani, Z.},
    pages = {1425--1432},
    posted-at = {2018-04-27 00:25:06},
    priority = {2},
    publisher = {MIT Press},
    title = {Learning a Gaussian Process Prior for Automatically Generating Music Playlists},
    url = {http://papers.nips.cc/paper/1996-learning-a-gaussian-process-prior-for-automatically-generating-music-playlists.pdf},
    year = {2002}
}

@misc{Olah2015Understanding,
    author = {Olah, Christopher},
    citeulike-article-id = {14362424},
    citeulike-linkout-0 = {https://colah.github.io/posts/2015-08-Understanding-LSTMs/},
    day = {27},
    howpublished = {Technical Blog},
    month = aug,
    posted-at = {2018-04-26 23:03:29},
    priority = {2},
    title = {Understanding {LSTM} Networks},
    url = {https://colah.github.io/posts/2015-08-Understanding-LSTMs/},
    year = {2015}
}

@misc{Gal2016Theoretically,
    abstract = {Recurrent neural networks ({RNNs}) stand at the forefront of many recent
developments in deep learning. Yet a major difficulty with these models is
their tendency to overfit, with dropout shown to fail when applied to recurrent
layers. Recent results at the intersection of Bayesian modelling and deep
learning offer a Bayesian interpretation of common deep learning techniques
such as dropout. This grounding of dropout in approximate Bayesian inference
suggests an extension of the theoretical results, offering insights into the
use of dropout with {RNN} models. We apply this new variational inference based
dropout technique in {LSTM} and {GRU} models, assessing it on language modelling
and sentiment analysis tasks. The new approach outperforms existing techniques,
and to the best of our knowledge improves on the single model state-of-the-art
in language modelling with the Penn Treebank (73.4 test perplexity). This
extends our arsenal of variational tools in deep learning.},
    archivePrefix = {arXiv},
    author = {Gal, Yarin and Ghahramani, Zoubin},
    citeulike-article-id = {14575931},
    citeulike-linkout-0 = {http://arxiv.org/abs/1512.05287.pdf},
    citeulike-linkout-1 = {http://arxiv.org/pdf/1512.05287.pdf},
    day = {5},
    eprint = {1512.05287.pdf},
    month = oct,
    posted-at = {2018-04-26 22:53:50},
    priority = {2},
    school = {University of Cambridge},
    title = {A Theoretically Grounded Application of Dropout in Recurrent Neural Networks},
    url = {http://arxiv.org/abs/1512.05287.pdf},
    year = {2016}
}

@misc{2017Adam,
    abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization
of stochastic objective functions, based on adaptive estimates of lower-order
moments. The method is straightforward to implement, is computationally
efficient, has little memory requirements, is invariant to diagonal rescaling
of the gradients, and is well suited for problems that are large in terms of
data and/or parameters. The method is also appropriate for non-stationary
objectives and problems with very noisy and/or sparse gradients. The
hyper-parameters have intuitive interpretations and typically require little
tuning. Some connections to related algorithms, on which Adam was inspired, are
discussed. We also analyze the theoretical convergence properties of the
algorithm and provide a regret bound on the convergence rate that is comparable
to the best known results under the online convex optimization framework.
Empirical results demonstrate that Adam works well in practice and compares
favorably to other stochastic optimization methods. Finally, we discuss {AdaMax},
a variant of Adam based on the infinity norm.},
    archivePrefix = {arXiv},
    citeulike-article-id = {14486792},
    citeulike-linkout-0 = {http://arxiv.org/abs/1412.6980.pdf},
    citeulike-linkout-1 = {http://arxiv.org/pdf/1412.6980.pdf},
    day = {30},
    eprint = {1412.6980.pdf},
    month = jan,
    posted-at = {2018-04-26 22:53:35},
    priority = {2},
    title = {Adam: A Method for Stochastic Optimization},
    url = {http://arxiv.org/abs/1412.6980.pdf},
    year = {2017}
}

@manual{2018Spotify,
    citeulike-article-id = {14575929},
    citeulike-linkout-0 = {https://beta.developer.spotify.com/documentation/web-api/reference/tracks/get-audio-features/},
    institution = {Spotify AB},
    month = apr,
    posted-at = {2018-04-26 22:52:48},
    priority = {2},
    title = {Spotify Web {API} | Spotify for Developers},
    url = {https://beta.developer.spotify.com/documentation/web-api/reference/tracks/get-audio-features/},
    year = {2018}
}

